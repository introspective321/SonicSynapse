{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11227769,"sourceType":"datasetVersion","datasetId":7012818}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nTrains a two-tower system:\n  - Query Tower: DistilBERT-based (fine-tuneable)\n  - Song Tower: MLP on numeric columns (including multi-hot tags, etc.)\n\"\"\"\n\nimport pandas as pd\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertModel\n\n\nclass DistilBertQueryEncoder(nn.Module):\n    def __init__(self, embed_dim=128):\n        super(DistilBertQueryEncoder, self).__init__()\n        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.projection = nn.Linear(768, embed_dim)\n\n    def forward(self, text_list):\n        encoded = self.tokenizer(\n            text_list,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(self.projection.weight.device)  # Ensure tensors are on the same device\n        input_ids = encoded[\"input_ids\"]\n        attention_mask = encoded[\"attention_mask\"]\n\n        outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n        cls_token = outputs.last_hidden_state[:, 0, :]  # shape [batch_size, 768]\n        query_emb = self.projection(cls_token)          # shape [batch_size, embed_dim]\n        return query_emb\n\n\nclass SongEncoder(nn.Module):\n    def __init__(self, input_dim, embed_dim=128):\n        super(SongEncoder, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, embed_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)  # shape [batch_size, embed_dim]\n\n\ndef contrastive_loss(query_emb, pos_emb, neg_emb, margin=1.0):\n    \"\"\"\n    Margin-based contrastive approach using cosine similarity\n    \"\"\"\n    cos_sim = nn.CosineSimilarity(dim=-1)\n    pos_sim = cos_sim(query_emb, pos_emb)  # shape [batch_size]\n    neg_sim = cos_sim(query_emb, neg_emb)\n    loss_val = torch.relu(margin - pos_sim + neg_sim).mean()\n    return loss_val\n\n\nclass TripletDataset(Dataset):\n    def __init__(self, df, triplets):\n        super().__init__()\n        self.df = df\n        self.triplets = triplets\n\n    def __len__(self):\n        return len(self.triplets)\n\n    def __getitem__(self, idx):\n        query_text, pos_i, neg_i = self.triplets[idx]\n        pos_feats = self.get_song_features(pos_i)\n        neg_feats = self.get_song_features(neg_i)\n        return query_text, pos_feats, neg_feats\n\n    def get_song_features(self, row_idx):\n        row = self.df.iloc[row_idx]\n        feats = row.values.astype(np.float32)\n        return feats\n\n\ndef collate_fn(batch):\n    query_list = []\n    pos_list = []\n    neg_list = []\n    for (q, p, n) in batch:\n        query_list.append(q)\n        pos_list.append(p)\n        neg_list.append(n)\n\n    pos_arr = np.array(pos_list, dtype=np.float32)\n    neg_arr = np.array(neg_list, dtype=np.float32)\n    pos_tensor = torch.tensor(pos_arr, dtype=torch.float32)\n    neg_tensor = torch.tensor(neg_arr, dtype=torch.float32)\n    return query_list, pos_tensor, neg_tensor\n\n\ndef train_two_tower(\n    df_csv=\"data_processed/MusicInfo_tagged.csv\",\n    triplets_pkl=\"triplets.pkl\",\n    embed_dim=128,\n    margin=1.0,\n    batch_size=8,\n    lr=1e-4,\n    num_epochs=3,\n    output_query_path=\"query_encoder_finetuned.pth\",\n    output_song_path=\"song_encoder.pth\"\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n\n    df = pd.read_csv(df_csv)\n    df.fillna(0, inplace=True)\n\n    input_dim = len(df.columns)\n    print(f\"Loaded numeric DataFrame with shape: {df.shape}, input_dim={input_dim}\")\n\n    with open(triplets_pkl, \"rb\") as f:\n        triplets = pickle.load(f)\n    print(f\"Loaded {len(triplets)} triplets from {triplets_pkl}\")\n\n    query_encoder = DistilBertQueryEncoder(embed_dim=embed_dim).to(device)\n    song_encoder = SongEncoder(input_dim=input_dim, embed_dim=embed_dim).to(device)\n\n    dataset = TripletDataset(df, triplets)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n    optimizer = optim.Adam(\n        list(query_encoder.parameters()) + list(song_encoder.parameters()),\n        lr=lr\n    )\n\n    for epoch in range(num_epochs):\n        query_encoder.train()\n        song_encoder.train()\n\n        total_loss = 0.0\n        print(f\"Starting epoch {epoch + 1}/{num_epochs}...\")\n        for i, (query_list, pos_feats, neg_feats) in enumerate(loader):\n            pos_feats = pos_feats.to(device)\n            neg_feats = neg_feats.to(device)\n\n            optimizer.zero_grad()\n\n            query_emb = query_encoder(query_list)  # shape [batch_size, embed_dim]\n            query_emb = query_emb.to(device)\n\n            pos_emb = song_encoder(pos_feats)  # shape [batch_size, embed_dim]\n            neg_emb = song_encoder(neg_feats)  # shape [batch_size, embed_dim]\n\n            loss_val = contrastive_loss(query_emb, pos_emb, neg_emb, margin=margin)\n            loss_val.backward()\n            optimizer.step()\n\n            total_loss += loss_val.item()\n\n            if (i + 1) % 10 == 0:  # Print every 10 batches\n                print(f\"Batch {i + 1}/{len(loader)}, Loss: {loss_val.item():.4f}\")\n\n        avg_loss = total_loss / len(loader)\n        print(f\"Epoch {epoch + 1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}\")\n\n    torch.save(query_encoder.state_dict(), output_query_path)\n    torch.save(song_encoder.state_dict(), output_song_path)\n    print(f\"Training complete. Saved query encoder -> {output_query_path}\")\n    print(f\"Training complete. Saved song encoder -> {output_song_path}\")\n\n\nif __name__ == \"__main__\":\n    train_two_tower(\n        df_csv=\"https://raw.githubusercontent.com/introspective321/SonicSynapse/refs/heads/main/data_processed/MusicInfo_tagged.csv\",\n        triplets_pkl=\"/kaggle/input/triplet/triplets.pkl\",\n        embed_dim=128,\n        margin=1.0,\n        batch_size=8,\n        lr=1e-4,\n        num_epochs=6,\n        output_query_path=\"query_encoder_finetuned.pth\",\n        output_song_path=\"song_encoder.pth\"\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:36:21.507247Z","iopub.execute_input":"2025-03-31T14:36:21.507543Z","iopub.status.idle":"2025-03-31T14:40:22.033490Z","shell.execute_reply.started":"2025-03-31T14:36:21.507516Z","shell.execute_reply":"2025-03-31T14:40:22.032328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded numeric DataFrame with shape: (50683, 117), input_dim=117\nLoaded 8000 triplets from /kaggle/input/triplet/triplets.pkl\nStarting epoch 1/6...\nBatch 10/1000, Loss: 0.9788\nBatch 20/1000, Loss: 1.0536\nBatch 30/1000, Loss: 0.9085\nBatch 40/1000, Loss: 0.6226\nBatch 50/1000, Loss: 0.6138\nBatch 60/1000, Loss: 0.7629\nBatch 70/1000, Loss: 0.4406\nBatch 80/1000, Loss: 0.6937\nBatch 90/1000, Loss: 0.5116\nBatch 100/1000, Loss: 0.3272\nBatch 110/1000, Loss: 0.5699\nBatch 120/1000, Loss: 0.5376\nBatch 130/1000, Loss: 0.4014\nBatch 140/1000, Loss: 0.5422\nBatch 150/1000, Loss: 0.3534\nBatch 160/1000, Loss: 0.2972\nBatch 170/1000, Loss: 0.2625\nBatch 180/1000, Loss: 0.3927\nBatch 190/1000, Loss: 0.3432\nBatch 200/1000, Loss: 0.2266\nBatch 210/1000, Loss: 0.2842\nBatch 220/1000, Loss: 0.5541\nBatch 230/1000, Loss: 0.2867\nBatch 240/1000, Loss: 0.1459\nBatch 250/1000, Loss: 0.3327\nBatch 260/1000, Loss: 0.4037\nBatch 270/1000, Loss: 0.2064\nBatch 280/1000, Loss: 0.2507\nBatch 290/1000, Loss: 0.2470\nBatch 300/1000, Loss: 0.2801\nBatch 310/1000, Loss: 0.1232\nBatch 320/1000, Loss: 0.1980\nBatch 330/1000, Loss: 0.4100\nBatch 340/1000, Loss: 0.1040\nBatch 350/1000, Loss: 0.2044\nBatch 360/1000, Loss: 0.3044\nBatch 370/1000, Loss: 0.2338\nBatch 380/1000, Loss: 0.3340\nBatch 390/1000, Loss: 0.1430\nBatch 400/1000, Loss: 0.0674\nBatch 410/1000, Loss: 0.2714\nBatch 420/1000, Loss: 0.2077\nBatch 430/1000, Loss: 0.3453\nBatch 440/1000, Loss: 0.2317\nBatch 450/1000, Loss: 0.3435\nBatch 460/1000, Loss: 0.2955\nBatch 470/1000, Loss: 0.1030\nBatch 480/1000, Loss: 0.3297\nBatch 490/1000, Loss: 0.2033\nBatch 500/1000, Loss: 0.0412\nBatch 510/1000, Loss: 0.2996\nBatch 520/1000, Loss: 0.1403\nBatch 530/1000, Loss: 0.1369\nBatch 540/1000, Loss: 0.2699\nBatch 550/1000, Loss: 0.2864\nBatch 560/1000, Loss: 0.3439\nBatch 570/1000, Loss: 0.2407\nBatch 580/1000, Loss: 0.2791\nBatch 590/1000, Loss: 0.0835\nBatch 600/1000, Loss: 0.2333\nBatch 610/1000, Loss: 0.1979\nBatch 620/1000, Loss: 0.3674\nBatch 630/1000, Loss: 0.2548\nBatch 640/1000, Loss: 0.2054\nBatch 650/1000, Loss: 0.0289\nBatch 660/1000, Loss: 0.1700\nBatch 670/1000, Loss: 0.1572\nBatch 680/1000, Loss: 0.1315\nBatch 690/1000, Loss: 0.2034\nBatch 700/1000, Loss: 0.1259\nBatch 710/1000, Loss: 0.1853\nBatch 720/1000, Loss: 0.4905\nBatch 730/1000, Loss: 0.2796\nBatch 740/1000, Loss: 0.1578\nBatch 750/1000, Loss: 0.0319\nBatch 760/1000, Loss: 0.1683\nBatch 770/1000, Loss: 0.1948\nBatch 780/1000, Loss: 0.2077\nBatch 790/1000, Loss: 0.1648\nBatch 800/1000, Loss: 0.1369\nBatch 810/1000, Loss: 0.2080\nBatch 820/1000, Loss: 0.2479\nBatch 830/1000, Loss: 0.4117\nBatch 840/1000, Loss: 0.2573\nBatch 850/1000, Loss: 0.1577\nBatch 860/1000, Loss: 0.1218\nBatch 870/1000, Loss: 0.1595\nBatch 880/1000, Loss: 0.1523\nBatch 890/1000, Loss: 0.1039\nBatch 900/1000, Loss: 0.3155\nBatch 910/1000, Loss: 0.2719\nBatch 920/1000, Loss: 0.1795\nBatch 930/1000, Loss: 0.1973\nBatch 940/1000, Loss: 0.2593\nBatch 950/1000, Loss: 0.1731\nBatch 960/1000, Loss: 0.1928\nBatch 970/1000, Loss: 0.1283\nBatch 980/1000, Loss: 0.1773\nBatch 990/1000, Loss: 0.0257\nBatch 1000/1000, Loss: 0.2566\nEpoch 1/6 completed. Average Loss: 0.3004\nStarting epoch 2/6...\nBatch 10/1000, Loss: 0.1816\nBatch 20/1000, Loss: 0.2502\nBatch 30/1000, Loss: 0.1696\nBatch 40/1000, Loss: 0.0484\nBatch 50/1000, Loss: 0.2803\nBatch 60/1000, Loss: 0.0799\nBatch 70/1000, Loss: 0.0725\nBatch 80/1000, Loss: 0.0761\nBatch 90/1000, Loss: 0.3044\nBatch 100/1000, Loss: 0.2047\nBatch 110/1000, Loss: 0.2249\nBatch 120/1000, Loss: 0.1679\nBatch 130/1000, Loss: 0.2164\nBatch 140/1000, Loss: 0.1084\nBatch 150/1000, Loss: 0.2230\nBatch 160/1000, Loss: 0.1746\nBatch 170/1000, Loss: 0.1420\nBatch 180/1000, Loss: 0.2335\nBatch 190/1000, Loss: 0.0694\nBatch 200/1000, Loss: 0.3580\nBatch 210/1000, Loss: 0.2697\nBatch 220/1000, Loss: 0.1001\nBatch 230/1000, Loss: 0.0364\nBatch 240/1000, Loss: 0.0783\nBatch 250/1000, Loss: 0.2836\nBatch 260/1000, Loss: 0.0639\nBatch 270/1000, Loss: 0.1624\nBatch 280/1000, Loss: 0.0828\nBatch 290/1000, Loss: 0.0708\nBatch 300/1000, Loss: 0.1771\nBatch 310/1000, Loss: 0.0334\nBatch 320/1000, Loss: 0.0788\nBatch 330/1000, Loss: 0.0074\nBatch 340/1000, Loss: 0.1346\nBatch 350/1000, Loss: 0.0716\nBatch 360/1000, Loss: 0.1607\nBatch 370/1000, Loss: 0.1819\nBatch 380/1000, Loss: 0.1414\nBatch 390/1000, Loss: 0.0977\nBatch 400/1000, Loss: 0.1813\nBatch 410/1000, Loss: 0.0831\nBatch 420/1000, Loss: 0.1374\nBatch 430/1000, Loss: 0.3208\nBatch 440/1000, Loss: 0.0496\nBatch 450/1000, Loss: 0.0789\nBatch 460/1000, Loss: 0.0451\nBatch 470/1000, Loss: 0.1868\nBatch 480/1000, Loss: 0.2245\nBatch 490/1000, Loss: 0.1602\nBatch 500/1000, Loss: 0.1602\nBatch 510/1000, Loss: 0.1665\nBatch 520/1000, Loss: 0.2040\nBatch 530/1000, Loss: 0.1790\nBatch 540/1000, Loss: 0.2643\nBatch 550/1000, Loss: 0.1109\nBatch 560/1000, Loss: 0.0719\nBatch 570/1000, Loss: 0.2358\nBatch 580/1000, Loss: 0.1298\nBatch 590/1000, Loss: 0.0652\nBatch 600/1000, Loss: 0.1539\nBatch 610/1000, Loss: 0.1158\nBatch 620/1000, Loss: 0.2288\nBatch 630/1000, Loss: 0.2509\nBatch 640/1000, Loss: 0.1365\nBatch 650/1000, Loss: 0.0121\nBatch 660/1000, Loss: 0.0040\nBatch 670/1000, Loss: 0.1699\nBatch 680/1000, Loss: 0.0698\nBatch 690/1000, Loss: 0.1431\nBatch 700/1000, Loss: 0.0230\nBatch 710/1000, Loss: 0.2137\nBatch 720/1000, Loss: 0.1241\nBatch 730/1000, Loss: 0.0426\nBatch 740/1000, Loss: 0.0063\nBatch 750/1000, Loss: 0.1339\nBatch 760/1000, Loss: 0.0681\nBatch 770/1000, Loss: 0.1855\nBatch 780/1000, Loss: 0.1457\nBatch 790/1000, Loss: 0.1696\nBatch 800/1000, Loss: 0.1514\nBatch 810/1000, Loss: 0.0821\nBatch 820/1000, Loss: 0.1635\nBatch 830/1000, Loss: 0.2097\nBatch 840/1000, Loss: 0.0870\nBatch 850/1000, Loss: 0.1106\nBatch 860/1000, Loss: 0.0893\nBatch 870/1000, Loss: 0.1113\nBatch 880/1000, Loss: 0.2131\nBatch 890/1000, Loss: 0.1843\nBatch 900/1000, Loss: 0.1535\nBatch 910/1000, Loss: 0.0538\nBatch 920/1000, Loss: 0.0942\nBatch 930/1000, Loss: 0.0151\nBatch 940/1000, Loss: 0.0702\nBatch 950/1000, Loss: 0.1812\nBatch 960/1000, Loss: 0.2498\nBatch 970/1000, Loss: 0.0875\nBatch 980/1000, Loss: 0.0922\nBatch 990/1000, Loss: 0.1120\nBatch 1000/1000, Loss: 0.1100\nEpoch 2/6 completed. Average Loss: 0.1419\nStarting epoch 3/6...\nBatch 10/1000, Loss: 0.0552\nBatch 20/1000, Loss: 0.2127\nBatch 30/1000, Loss: 0.0104\nBatch 40/1000, Loss: 0.1289\nBatch 50/1000, Loss: 0.0643\nBatch 60/1000, Loss: 0.0655\nBatch 70/1000, Loss: 0.1066\nBatch 80/1000, Loss: 0.0893\nBatch 90/1000, Loss: 0.0185\nBatch 100/1000, Loss: 0.0657\nBatch 110/1000, Loss: 0.1331\nBatch 120/1000, Loss: 0.0825\nBatch 130/1000, Loss: 0.1425\nBatch 140/1000, Loss: 0.1519\nBatch 150/1000, Loss: 0.0962\nBatch 160/1000, Loss: 0.0127\nBatch 170/1000, Loss: 0.0792\nBatch 180/1000, Loss: 0.1466\nBatch 190/1000, Loss: 0.2194\nBatch 200/1000, Loss: 0.2316\nBatch 210/1000, Loss: 0.0987\nBatch 220/1000, Loss: 0.0422\nBatch 230/1000, Loss: 0.0542\nBatch 240/1000, Loss: 0.1076\nBatch 250/1000, Loss: 0.0677\nBatch 260/1000, Loss: 0.1203\nBatch 270/1000, Loss: 0.0503\nBatch 280/1000, Loss: 0.1167\nBatch 290/1000, Loss: 0.0641\nBatch 300/1000, Loss: 0.0851\nBatch 310/1000, Loss: 0.1830\nBatch 320/1000, Loss: 0.2359\nBatch 330/1000, Loss: 0.1464\nBatch 340/1000, Loss: 0.0295\nBatch 350/1000, Loss: 0.0806\nBatch 360/1000, Loss: 0.1487\nBatch 370/1000, Loss: 0.0531\nBatch 380/1000, Loss: 0.0675\nBatch 390/1000, Loss: 0.1215\nBatch 400/1000, Loss: 0.1927\nBatch 410/1000, Loss: 0.0182\nBatch 420/1000, Loss: 0.0807\nBatch 430/1000, Loss: 0.0723\nBatch 440/1000, Loss: 0.0596\nBatch 450/1000, Loss: 0.0210\nBatch 460/1000, Loss: 0.0866\nBatch 470/1000, Loss: 0.0721\nBatch 480/1000, Loss: 0.1118\nBatch 490/1000, Loss: 0.0582\nBatch 500/1000, Loss: 0.0423\nBatch 510/1000, Loss: 0.0576\nBatch 520/1000, Loss: 0.0491\nBatch 530/1000, Loss: 0.0665\nBatch 540/1000, Loss: 0.0868\nBatch 550/1000, Loss: 0.1367\nBatch 560/1000, Loss: 0.1006\nBatch 570/1000, Loss: 0.0394\nBatch 580/1000, Loss: 0.1208\nBatch 590/1000, Loss: 0.0548\nBatch 600/1000, Loss: 0.0719\nBatch 610/1000, Loss: 0.2272\nBatch 620/1000, Loss: 0.0603\nBatch 630/1000, Loss: 0.0691\nBatch 640/1000, Loss: 0.0414\nBatch 650/1000, Loss: 0.1257\nBatch 660/1000, Loss: 0.1481\nBatch 670/1000, Loss: 0.0596\nBatch 680/1000, Loss: 0.0712\nBatch 690/1000, Loss: 0.0788\nBatch 700/1000, Loss: 0.1667\nBatch 710/1000, Loss: 0.1722\nBatch 720/1000, Loss: 0.0938\nBatch 730/1000, Loss: 0.1280\nBatch 740/1000, Loss: 0.0308\nBatch 750/1000, Loss: 0.0650\nBatch 760/1000, Loss: 0.0332\nBatch 770/1000, Loss: 0.0383\nBatch 780/1000, Loss: 0.1701\nBatch 790/1000, Loss: 0.0418\nBatch 800/1000, Loss: 0.0824\nBatch 810/1000, Loss: 0.1231\nBatch 820/1000, Loss: 0.0649\nBatch 830/1000, Loss: 0.0804\nBatch 840/1000, Loss: 0.0388\nBatch 850/1000, Loss: 0.1312\nBatch 860/1000, Loss: 0.1554\nBatch 870/1000, Loss: 0.2251\nBatch 880/1000, Loss: 0.1097\nBatch 890/1000, Loss: 0.0884\nBatch 900/1000, Loss: 0.0617\nBatch 910/1000, Loss: 0.0624\nBatch 920/1000, Loss: 0.0073\nBatch 930/1000, Loss: 0.0769\nBatch 940/1000, Loss: 0.1015\nBatch 950/1000, Loss: 0.0889\nBatch 960/1000, Loss: 0.0250\nBatch 970/1000, Loss: 0.1209\nBatch 980/1000, Loss: 0.1387\nBatch 990/1000, Loss: 0.0374\nBatch 1000/1000, Loss: 0.0783\nEpoch 3/6 completed. Average Loss: 0.1072\nStarting epoch 4/6...\nBatch 10/1000, Loss: 0.0973\nBatch 20/1000, Loss: 0.0675\nBatch 30/1000, Loss: 0.1455\nBatch 40/1000, Loss: 0.0384\nBatch 50/1000, Loss: 0.0517\nBatch 60/1000, Loss: 0.0551\nBatch 70/1000, Loss: 0.0403\nBatch 80/1000, Loss: 0.1168\nBatch 90/1000, Loss: 0.1133\nBatch 100/1000, Loss: 0.1388\nBatch 110/1000, Loss: 0.1204\nBatch 120/1000, Loss: 0.0016\nBatch 130/1000, Loss: 0.1298\nBatch 140/1000, Loss: 0.1854\nBatch 150/1000, Loss: 0.0199\nBatch 160/1000, Loss: 0.0656\nBatch 170/1000, Loss: 0.0523\nBatch 180/1000, Loss: 0.0121\nBatch 190/1000, Loss: 0.0208\nBatch 200/1000, Loss: 0.1694\nBatch 210/1000, Loss: 0.0191\nBatch 220/1000, Loss: 0.0778\nBatch 230/1000, Loss: 0.0501\nBatch 240/1000, Loss: 0.0000\nBatch 250/1000, Loss: 0.1245\nBatch 260/1000, Loss: 0.0883\nBatch 270/1000, Loss: 0.1363\nBatch 280/1000, Loss: 0.0500\nBatch 290/1000, Loss: 0.1148\nBatch 300/1000, Loss: 0.1116\nBatch 310/1000, Loss: 0.0658\nBatch 320/1000, Loss: 0.0436\nBatch 330/1000, Loss: 0.1075\nBatch 340/1000, Loss: 0.0133\nBatch 350/1000, Loss: 0.0189\nBatch 360/1000, Loss: 0.0400\nBatch 370/1000, Loss: 0.0162\nBatch 380/1000, Loss: 0.0538\nBatch 390/1000, Loss: 0.0520\nBatch 400/1000, Loss: 0.0777\nBatch 410/1000, Loss: 0.0643\nBatch 420/1000, Loss: 0.1496\nBatch 430/1000, Loss: 0.0895\nBatch 440/1000, Loss: 0.0357\nBatch 450/1000, Loss: 0.0318\nBatch 460/1000, Loss: 0.1442\nBatch 470/1000, Loss: 0.1116\nBatch 480/1000, Loss: 0.0769\nBatch 490/1000, Loss: 0.0837\nBatch 500/1000, Loss: 0.0326\nBatch 510/1000, Loss: 0.1017\nBatch 520/1000, Loss: 0.0482\nBatch 530/1000, Loss: 0.0147\nBatch 540/1000, Loss: 0.0370\nBatch 550/1000, Loss: 0.1059\nBatch 560/1000, Loss: 0.0689\nBatch 570/1000, Loss: 0.1105\nBatch 580/1000, Loss: 0.0848\nBatch 590/1000, Loss: 0.0292\nBatch 600/1000, Loss: 0.1419\nBatch 610/1000, Loss: 0.1715\nBatch 620/1000, Loss: 0.0312\nBatch 630/1000, Loss: 0.1280\nBatch 640/1000, Loss: 0.0831\nBatch 650/1000, Loss: 0.0845\nBatch 660/1000, Loss: 0.0722\nBatch 670/1000, Loss: 0.0460\nBatch 680/1000, Loss: 0.0537\nBatch 690/1000, Loss: 0.0324\nBatch 700/1000, Loss: 0.0663\nBatch 710/1000, Loss: 0.1844\nBatch 720/1000, Loss: 0.1000\nBatch 730/1000, Loss: 0.1745\nBatch 740/1000, Loss: 0.2722\nBatch 750/1000, Loss: 0.1987\nBatch 760/1000, Loss: 0.1607\nBatch 770/1000, Loss: 0.0663\nBatch 780/1000, Loss: 0.0925\nBatch 790/1000, Loss: 0.0752\nBatch 800/1000, Loss: 0.0077\nBatch 810/1000, Loss: 0.0318\nBatch 820/1000, Loss: 0.1027\nBatch 830/1000, Loss: 0.0605\nBatch 840/1000, Loss: 0.2379\nBatch 850/1000, Loss: 0.0530\nBatch 860/1000, Loss: 0.0330\nBatch 870/1000, Loss: 0.1010\nBatch 880/1000, Loss: 0.0072\nBatch 890/1000, Loss: 0.1524\nBatch 900/1000, Loss: 0.1273\nBatch 910/1000, Loss: 0.0980\nBatch 920/1000, Loss: 0.0839\nBatch 930/1000, Loss: 0.1541\nBatch 940/1000, Loss: 0.0780\nBatch 950/1000, Loss: 0.1286\nBatch 960/1000, Loss: 0.0458\nBatch 970/1000, Loss: 0.0689\nBatch 980/1000, Loss: 0.0875\nBatch 990/1000, Loss: 0.1194\nBatch 1000/1000, Loss: 0.0468\nEpoch 4/6 completed. Average Loss: 0.0869\nStarting epoch 5/6...\nBatch 10/1000, Loss: 0.0358\nBatch 20/1000, Loss: 0.0000\nBatch 30/1000, Loss: 0.0205\nBatch 40/1000, Loss: 0.0876\nBatch 50/1000, Loss: 0.0701\nBatch 60/1000, Loss: 0.0787\nBatch 70/1000, Loss: 0.0164\nBatch 80/1000, Loss: 0.0213\nBatch 90/1000, Loss: 0.0205\nBatch 100/1000, Loss: 0.0178\nBatch 110/1000, Loss: 0.0135\nBatch 120/1000, Loss: 0.0520\nBatch 130/1000, Loss: 0.1221\nBatch 140/1000, Loss: 0.0105\nBatch 150/1000, Loss: 0.0853\nBatch 160/1000, Loss: 0.0000\nBatch 170/1000, Loss: 0.1074\nBatch 180/1000, Loss: 0.0565\nBatch 190/1000, Loss: 0.0811\nBatch 200/1000, Loss: 0.1595\nBatch 210/1000, Loss: 0.1286\nBatch 220/1000, Loss: 0.1386\nBatch 230/1000, Loss: 0.0239\nBatch 240/1000, Loss: 0.0733\nBatch 250/1000, Loss: 0.0508\nBatch 260/1000, Loss: 0.0475\nBatch 270/1000, Loss: 0.0232\nBatch 280/1000, Loss: 0.1585\nBatch 290/1000, Loss: 0.0926\nBatch 300/1000, Loss: 0.2268\nBatch 310/1000, Loss: 0.1949\nBatch 320/1000, Loss: 0.0050\nBatch 330/1000, Loss: 0.0915\nBatch 340/1000, Loss: 0.0460\nBatch 350/1000, Loss: 0.0949\nBatch 360/1000, Loss: 0.0984\nBatch 370/1000, Loss: 0.0529\nBatch 380/1000, Loss: 0.0222\nBatch 390/1000, Loss: 0.1206\nBatch 400/1000, Loss: 0.0000\nBatch 410/1000, Loss: 0.0769\nBatch 420/1000, Loss: 0.0751\nBatch 430/1000, Loss: 0.0129\nBatch 440/1000, Loss: 0.1050\nBatch 450/1000, Loss: 0.0272\nBatch 460/1000, Loss: 0.1349\nBatch 470/1000, Loss: 0.0382\nBatch 480/1000, Loss: 0.0826\nBatch 490/1000, Loss: 0.1270\nBatch 500/1000, Loss: 0.0087\nBatch 510/1000, Loss: 0.1220\nBatch 520/1000, Loss: 0.0302\nBatch 530/1000, Loss: 0.1743\nBatch 540/1000, Loss: 0.0577\nBatch 550/1000, Loss: 0.1109\nBatch 560/1000, Loss: 0.0654\nBatch 570/1000, Loss: 0.0963\nBatch 580/1000, Loss: 0.1559\nBatch 590/1000, Loss: 0.0297\nBatch 600/1000, Loss: 0.0288\nBatch 610/1000, Loss: 0.0757\nBatch 620/1000, Loss: 0.0658\nBatch 630/1000, Loss: 0.0500\nBatch 640/1000, Loss: 0.0752\nBatch 650/1000, Loss: 0.2218\nBatch 660/1000, Loss: 0.0254\nBatch 670/1000, Loss: 0.0272\nBatch 680/1000, Loss: 0.0639\nBatch 690/1000, Loss: 0.0592\nBatch 700/1000, Loss: 0.0773\nBatch 710/1000, Loss: 0.0490\nBatch 720/1000, Loss: 0.0159\nBatch 730/1000, Loss: 0.0567\nBatch 740/1000, Loss: 0.1487\nBatch 750/1000, Loss: 0.0521\nBatch 760/1000, Loss: 0.0451\nBatch 770/1000, Loss: 0.0365\nBatch 780/1000, Loss: 0.1178\nBatch 790/1000, Loss: 0.0299\nBatch 800/1000, Loss: 0.0067\nBatch 810/1000, Loss: 0.0546\nBatch 820/1000, Loss: 0.0485\nBatch 830/1000, Loss: 0.0176\nBatch 840/1000, Loss: 0.1057\nBatch 850/1000, Loss: 0.0560\nBatch 860/1000, Loss: 0.0254\nBatch 870/1000, Loss: 0.0509\nBatch 880/1000, Loss: 0.0111\nBatch 890/1000, Loss: 0.0136\nBatch 900/1000, Loss: 0.0533\nBatch 910/1000, Loss: 0.1144\nBatch 920/1000, Loss: 0.0115\nBatch 930/1000, Loss: 0.0895\nBatch 940/1000, Loss: 0.0746\nBatch 950/1000, Loss: 0.1208\nBatch 960/1000, Loss: 0.1670\nBatch 970/1000, Loss: 0.0602\nBatch 980/1000, Loss: 0.0081\nBatch 990/1000, Loss: 0.0794\nBatch 1000/1000, Loss: 0.0427\nEpoch 5/6 completed. Average Loss: 0.0734\nStarting epoch 6/6...\nBatch 10/1000, Loss: 0.1199\nBatch 20/1000, Loss: 0.0119\nBatch 30/1000, Loss: 0.0222\nBatch 40/1000, Loss: 0.1101\nBatch 50/1000, Loss: 0.1091\nBatch 60/1000, Loss: 0.0427\nBatch 70/1000, Loss: 0.0977\nBatch 80/1000, Loss: 0.0383\nBatch 90/1000, Loss: 0.0275\nBatch 100/1000, Loss: 0.0477\nBatch 110/1000, Loss: 0.1655\nBatch 120/1000, Loss: 0.0361\nBatch 130/1000, Loss: 0.1724\nBatch 140/1000, Loss: 0.0094\nBatch 150/1000, Loss: 0.0969\nBatch 160/1000, Loss: 0.0482\nBatch 170/1000, Loss: 0.0077\nBatch 180/1000, Loss: 0.0215\nBatch 190/1000, Loss: 0.1183\nBatch 200/1000, Loss: 0.0247\nBatch 210/1000, Loss: 0.0448\nBatch 220/1000, Loss: 0.1409\nBatch 230/1000, Loss: 0.0353\nBatch 240/1000, Loss: 0.0147\nBatch 250/1000, Loss: 0.0408\nBatch 260/1000, Loss: 0.0352\nBatch 270/1000, Loss: 0.0309\nBatch 280/1000, Loss: 0.1456\nBatch 290/1000, Loss: 0.0474\nBatch 300/1000, Loss: 0.0089\nBatch 310/1000, Loss: 0.0654\nBatch 320/1000, Loss: 0.2277\nBatch 330/1000, Loss: 0.0699\nBatch 340/1000, Loss: 0.0494\nBatch 350/1000, Loss: 0.0341\nBatch 360/1000, Loss: 0.1095\nBatch 370/1000, Loss: 0.0279\nBatch 380/1000, Loss: 0.0850\nBatch 390/1000, Loss: 0.0131\nBatch 400/1000, Loss: 0.0955\nBatch 410/1000, Loss: 0.0111\nBatch 420/1000, Loss: 0.0111\nBatch 430/1000, Loss: 0.0923\nBatch 440/1000, Loss: 0.0062\nBatch 450/1000, Loss: 0.1190\nBatch 460/1000, Loss: 0.1093\nBatch 470/1000, Loss: 0.0231\nBatch 480/1000, Loss: 0.0123\nBatch 490/1000, Loss: 0.0932\nBatch 500/1000, Loss: 0.0245\nBatch 510/1000, Loss: 0.0758\nBatch 520/1000, Loss: 0.0316\nBatch 530/1000, Loss: 0.0548\nBatch 540/1000, Loss: 0.0604\nBatch 550/1000, Loss: 0.0411\nBatch 560/1000, Loss: 0.0953\nBatch 570/1000, Loss: 0.1150\nBatch 580/1000, Loss: 0.0323\nBatch 590/1000, Loss: 0.0564\nBatch 600/1000, Loss: 0.0187\nBatch 610/1000, Loss: 0.0807\nBatch 620/1000, Loss: 0.0659\nBatch 630/1000, Loss: 0.1508\nBatch 640/1000, Loss: 0.0764\nBatch 650/1000, Loss: 0.1182\nBatch 660/1000, Loss: 0.0826\nBatch 670/1000, Loss: 0.0358\nBatch 680/1000, Loss: 0.0614\nBatch 690/1000, Loss: 0.0276\nBatch 700/1000, Loss: 0.0386\nBatch 710/1000, Loss: 0.0668\nBatch 720/1000, Loss: 0.0351\nBatch 730/1000, Loss: 0.0416\nBatch 740/1000, Loss: 0.0779\nBatch 750/1000, Loss: 0.0494\nBatch 760/1000, Loss: 0.0623\nBatch 770/1000, Loss: 0.0286\nBatch 780/1000, Loss: 0.0177\nBatch 790/1000, Loss: 0.0435\nBatch 800/1000, Loss: 0.0921\nBatch 810/1000, Loss: 0.0894\nBatch 820/1000, Loss: 0.0518\nBatch 830/1000, Loss: 0.1320\nBatch 840/1000, Loss: 0.1555\nBatch 850/1000, Loss: 0.0619\nBatch 860/1000, Loss: 0.0287\nBatch 870/1000, Loss: 0.0473\nBatch 880/1000, Loss: 0.0149\nBatch 890/1000, Loss: 0.0344\nBatch 900/1000, Loss: 0.0028\nBatch 910/1000, Loss: 0.0299\nBatch 920/1000, Loss: 0.0549\nBatch 930/1000, Loss: 0.0341\nBatch 940/1000, Loss: 0.0692\nBatch 950/1000, Loss: 0.0748\nBatch 960/1000, Loss: 0.0120\nBatch 970/1000, Loss: 0.0281\nBatch 980/1000, Loss: 0.1047\nBatch 990/1000, Loss: 0.0114\nBatch 1000/1000, Loss: 0.0744\nEpoch 6/6 completed. Average Loss: 0.0628\nTraining complete. Saved query encoder -> query_encoder_finetuned.pth\nTraining complete. Saved song encoder -> song_encoder.pth\n","output_type":"stream"}],"execution_count":7}]}